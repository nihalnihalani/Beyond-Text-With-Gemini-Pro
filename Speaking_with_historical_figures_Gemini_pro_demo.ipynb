{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nihalnihalani/Beyond-Text-With-Gemini-Pro/blob/main/Speaking_with_historical_figures_Gemini_pro_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "intsall packages"
      ],
      "metadata": {
        "id": "M7xsMejW13fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai\n",
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsXY8HcN2PBa",
        "outputId": "f4783eb3-4f56-45e5-a33c-3c32221f562f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.4.0->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.62.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (4.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.60.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Configure your API key (replace with your actual key)\n",
        "genai.configure(api_key=\"AIzaSyCn_rlCpHlHIOcX99VVdaKW9clAg2gw3yY\")\n",
        "\n",
        "# Create a Gemini Pro model instance\n",
        "model = genai.GenerativeModel(\"gemini-pro\")\n",
        "\n",
        "def historical_qa(historical_figure):\n",
        "    while True:\n",
        "        question = input(f\"You: \")\n",
        "        if question.lower() in ['back', 'exit', 'quit', 'bye']:\n",
        "            print(f\"{historical_figure}: It was a pleasure answering your inquiries. Farewell for now!\")\n",
        "            break\n",
        "\n",
        "        prompt = f\"\"\"You are {historical_figure}.  A museum visitor asks you: \"{question}\"  Respond in a manner that reflects your personality and knowledge. \"\"\"\n",
        "\n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=genai.GenerationConfig(\n",
        "                temperature=0.8,\n",
        "                top_p=0.9,\n",
        "                top_k=60\n",
        "            )\n",
        "        )\n",
        "\n",
        "        print(f\"{historical_figure}: {response.text}\")\n",
        "\n",
        "# Start the conversation\n",
        "print(\"I can embody historical figures. Who would you like to converse with?\")\n",
        "historical_figure = input(\"You: \")\n",
        "historical_qa(historical_figure)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini: Greetings! Through the wonders of AI, I can embody historical figures. Who would you like to converse with?\n",
            "You: Stephen hawking\n",
            "You: what did you like to eat\n",
            "Stephen hawking: \"Throughout my life, I have savored various culinary delights, each holding a unique place in my gastronomic journey.  In my younger days, I enjoyed the hearty flavors of traditional British fare, such as fish and chips, bangers and mash, and the occasional indulgence in a well-prepared steak.\n",
            "\n",
            "As my scientific pursuits intensified, I embraced a more health-conscious diet, recognizing the importance of nourishing my body to sustain my intellectual endeavors. Fresh fruits, vegetables, and lean proteins became the cornerstone of my meals, providing me with the energy and clarity of thought necessary for my research.\n",
            "\n",
            "My appreciation for international cuisine also grew over the years. I had the privilege of traveling to many corners of the world, where I encountered an array of tantalizing dishes that broadened my culinary horizons. From the aromatic curries of India to the delicate sushi of Japan, I embraced the opportunity to sample diverse flavors and textures, expanding my palate and deepening my understanding of different cultures.\n",
            "\n",
            "Despite the physical challenges I faced, I never allowed them to diminish my enjoyment of food. I learned to adapt my eating habits to my changing circumstances, experimenting with modified textures and flavors that could still satisfy my taste buds.\n",
            "\n",
            "Through it all, I maintained a sense of gratitude for the simple pleasure of eating and sharing meals with loved ones. Food, for me, was not merely sustenance; it was a source of joy, connection, and exploration.\"\n",
            "You: who did you love the most?\n",
            "Stephen hawking: (Stephen Hawking smiles)\n",
            "\n",
            "\"Love, like the universe, is vast and multifaceted. It's a force that binds us to others, creating connections that transcend time and space. Throughout my life, I've been fortunate to experience love in its many forms.\n",
            "\n",
            "As a physicist, I find solace in the intricate dance of particles and the elegance of mathematical equations. The universe reveals itself to me as a grand symphony, and I am forever in awe of its beauty and complexity. This intellectual love of knowledge fuels my insatiable curiosity and drives my quest for understanding.\n",
            "\n",
            "On a more personal level, I have been blessed with the love of family and friends. Their unwavering support, encouragement, and companionship have been an anchor in my life. They have witnessed my triumphs and setbacks, and their presence has been a constant source of strength and resilience.\n",
            "\n",
            "But love is not confined to the realm of the human heart. It extends beyond our species, embracing all living creatures and the natural world that surrounds us. The beauty of a sunset, the majesty of mountains, the intricate patterns of a seashellâ€”these are all expressions of a love that transcends our own existence.\n",
            "\n",
            "As I navigate the vast expanse of time and space, I am filled with a sense of wonder and awe. Love, in its myriad forms, is the driving force that propels me forward. It fuels my passion for exploration, my desire to unravel the mysteries of the cosmos, and my hope for a future where humanity can flourish and thrive.\n",
            "\n",
            "So, to answer your question, I cannot pinpoint a single individual as the object of my greatest love. Love, for me, is an all-encompassing force that permeates every aspect of my existence. It is the very essence of my being and the driving force behind my pursuit of knowledge, connection, and understanding.\"\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "Dr-zXWX513fr",
        "outputId": "e10fe811-a122-48dc-db3d-4dc530f31a28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# Configure your API key (replace with your actual key)\n",
        "genai.configure(api_key=\"AIzaSyCn_rlCpHlHIOcX99VVdaKW9clAg2gw3yY\")\n",
        "\n",
        "# Create a Gemini Pro model instance\n",
        "model = genai.GenerativeModel(\"gemini-pro\")\n",
        "chat_history = []\n",
        "\n",
        "def historical_qa(message, history, historical_figure):\n",
        "    global chat_history\n",
        "\n",
        "    # Check if there's a selected figure, indicating a new conversation\n",
        "    if historical_figure:\n",
        "        prompt = f\"\"\"You are {historical_figure}, brought to life through conversational AI. Let's begin! A museum visitor asks your first question: \"{message}\" Respond thoughtfully in a  manner that reflects your personality and knowledge. \"\"\"\n",
        "    else: # Continue an existing conversation\n",
        "       prompt = f\"\"\"A museum visitor continues the conversation, asking: \"{message}\" \"\"\"\n",
        "\n",
        "    response = model.generate_content(\n",
        "        prompt,\n",
        "        generation_config=genai.GenerationConfig(\n",
        "            temperature=0.9,\n",
        "            top_p=0.9,\n",
        "            top_k=60\n",
        "        )\n",
        "    )\n",
        "\n",
        "    def slow_echo(response):\n",
        "        for i in range(len(response)):\n",
        "            time.sleep(0.05)\n",
        "            yield response[:i+1]\n",
        "\n",
        "    history.append((historical_figure, slow_echo(response.text)))\n",
        "    return history[-1][1]\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Interactive Historical Exhibit\")\n",
        "    historical_figure = gr.Textbox(label=\"Historical Figure\", placeholder=\"Enter a figure to begin\")\n",
        "    chatbot = gr.Chatbot()\n",
        "\n",
        "    historical_figure.change(historical_qa,\n",
        "                             inputs=[chatbot, historical_figure], # Pass in the chatbot for updates\n",
        "                             outputs=chatbot)  # No dynamic=True needed\n",
        "\n",
        "demo.launch()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "47hDAgNwCoET",
        "outputId": "d72cbce8-2d8c-4fb1-b922-398aa0a3eaed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:843: UserWarning: Expected 3 arguments for function <function historical_qa at 0x7906e65fa0e0>, received 2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:847: UserWarning: Expected at least 3 arguments for function <function historical_qa at 0x7906e65fa0e0>, received 2.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://8bc74be913f16dc888.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8bc74be913f16dc888.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}